{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f7700b-d6a4-420f-9f8b-be75b36311ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bf433f-e2f6-4139-a377-07f95236a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate labels and generate lookup table\n",
    "def concat_labels_generate_lookup(iRFP_path, PI_path):\n",
    "    '''\n",
    "    Parameters: iRFP670 labeled image path, PI labeled image path\n",
    "    Returns: concatenated labeled image (with each cell relabeled), lookup table with the source (Live or Dead) for each label\n",
    "    '''\n",
    "    # Load the images\n",
    "    iRFP = tifffile.imread(iRFP_path)\n",
    "    PI = tifffile.imread(PI_path)\n",
    "    \n",
    "    # Offset and concatenate\n",
    "    T, H, W = iRFP.shape\n",
    "    combined_stack = np.zeros((T, H, W), dtype=np.uint16)\n",
    "    \n",
    "    lookup_entries = []\n",
    "    \n",
    "    label_id_counter = 1  # start labeling from 1 (0 is background)\n",
    "    \n",
    "    for t in range(T):\n",
    "        iRFP_frame = iRFP[t]\n",
    "        PI_frame = PI[t]\n",
    "    \n",
    "        # Create unique labels per frame for cytoplasm\n",
    "        iRFP_labels = np.unique(iRFP_frame)\n",
    "        iRFP_labels = iRFP_labels[iRFP_labels > 0]  # exclude background\n",
    "    \n",
    "        for lbl in iRFP_labels:\n",
    "            mask = iRFP_frame == lbl\n",
    "            combined_stack[t][mask] = label_id_counter\n",
    "            lookup_entries.append({\n",
    "                \"FRAME\": t,\n",
    "                \"ID\": label_id_counter,\n",
    "                \"SOURCE\": \"Live\"\n",
    "            })\n",
    "            label_id_counter += 1\n",
    "    \n",
    "        # Create unique labels for nuclei (offset labels)\n",
    "        PI_labels = np.unique(PI_frame)\n",
    "        PI_labels = PI_labels[PI_labels > 0]\n",
    "    \n",
    "        for lbl in PI_labels:\n",
    "            mask = PI_frame == lbl\n",
    "            combined_stack[t][mask] = label_id_counter\n",
    "            lookup_entries.append({\n",
    "                \"FRAME\": t,\n",
    "                \"ID\": label_id_counter,\n",
    "                \"SOURCE\": \"Dead\"\n",
    "            })\n",
    "            label_id_counter += 1\n",
    "\n",
    "    return combined_stack, lookup_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9e7781-ccc0-4a09-aa37-4657a266527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-maps IDs in TrackMate output based on matching xy coordinates of labels in TrackMate output and original labeled image\n",
    "# Maps state of cell (Live or Dead) based on lookup table\n",
    "def spots_relabel(trackmate_path, label_image_path, lookup_path, pixel_size = 1):\n",
    "    '''\n",
    "    Parameters: TrackMate spots file path, labeled image path, lookup table path, (optional): micron-pixel ratio\n",
    "    Return: TrackMate spots file with cell labels updated to match labels in labeled image, with SOURCE column added\n",
    "    '''\n",
    "    # Load data\n",
    "    spots_df = pd.read_csv(trackmate_path)\n",
    "    spots_df.drop([0,1,2], axis=0, inplace=True) # Remove duplicated column names in from TrackMate's output\n",
    "    spots_df = spots_df.reset_index()\n",
    "    \n",
    "    # Cleanup data\n",
    "    spots_df['POSITION_X'] = pd.to_numeric(spots_df['POSITION_X'], errors='coerce')\n",
    "    spots_df['POSITION_Y'] = pd.to_numeric(spots_df['POSITION_Y'], errors='coerce')\n",
    "    spots_df['FRAME'] = pd.to_numeric(spots_df['FRAME'], errors='coerce')\n",
    "\n",
    "    # Drop rows where conversion failed (i.e., invalid entries became NaN)\n",
    "    spots_df.dropna(subset=['POSITION_X', 'POSITION_Y','FRAME'], inplace=True)\n",
    "    \n",
    "    label_stack = tifffile.imread(label_image_path)\n",
    "    lookup_df = pd.read_csv(lookup_path)\n",
    "    \n",
    "    # Convert xy positions in TrackMate output from microns to pixels\n",
    "    spots_df['X_px'] = (spots_df['POSITION_X'] / pixel_size).round().astype(int)\n",
    "    spots_df['Y_px'] = (spots_df['POSITION_Y'] / pixel_size).round().astype(int)\n",
    "    \n",
    "    # Get label ID at each (X, Y, FRAME)\n",
    "    def get_label_id(row):\n",
    "        t = row['FRAME']\n",
    "        y = row['Y_px']\n",
    "        x = row['X_px']\n",
    "        try:\n",
    "            return int(label_stack[t, y, x])\n",
    "        except IndexError:\n",
    "            return -1  # or np.nan\n",
    "    \n",
    "    spots_df['ID'] = spots_df.apply(get_label_id, axis=1)\n",
    "    \n",
    "    # Add source info the two tables based on frame and label ID\n",
    "    lookup_dict = {\n",
    "    (int(row['FRAME']), int(row['ID'])): row['SOURCE']\n",
    "    for _, row in lookup_df.iterrows()\n",
    "    }\n",
    "    # Create a dictionary with (FRAME, ID) as keys and source as value\n",
    "    def get_source(row):\n",
    "        key = (int(row['FRAME']), int(row['ID']))\n",
    "        return lookup_dict.get(key, 'unknown')  # default to 'unknown' if no match\n",
    "    # Apply it\n",
    "    spots_df['SOURCE'] = spots_df.apply(get_source, axis=1)\n",
    "    \n",
    "    return spots_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d112f97c-0ea5-4c04-bb12-dc0b4c28e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check for tracking result\n",
    "def tracking_quality_check_transition(spots_df):\n",
    "    \"\"\"\n",
    "    Given a TrackMate results DataFrame with 'Track_ID', 'FRAME', and 'SOURCE' columns,\n",
    "    detect tracks where a 'Dead' label appears before the last 'Live' label.\n",
    "    \n",
    "    Returns a list of problematic track IDs and a summary count.\n",
    "    \"\"\"\n",
    "    bad_tracks = []\n",
    "\n",
    "    for track_id, group in spots_df.groupby('TRACK_ID'):\n",
    "        sources = group.sort_values('FRAME')['SOURCE'].tolist()\n",
    "\n",
    "        # Find the last index of 'Live' (from iRFP670 channel)\n",
    "        try:\n",
    "            last_cyto_idx = len(sources) - 1 - sources[::-1].index('Live')\n",
    "        except ValueError:\n",
    "            last_cyto_idx = -1  # no cytoplasm at all\n",
    "\n",
    "        # Check if there's a 'Dead' (from PI channel) before that\n",
    "        if last_cyto_idx > 0 and 'Dead' in sources[:last_cyto_idx]:\n",
    "            bad_tracks.append(track_id)\n",
    "\n",
    "    return bad_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88fa3f7-13fa-4699-8b39-2e25fcd562e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check for tracking result\n",
    "def tracking_quality_check_dead(spots_df):\n",
    "    \"\"\"\n",
    "    Given a TrackMate results DataFrame with 'Track_ID', 'FRAME', and 'SOURCE' columns,\n",
    "    detect tracks where the entire track only has 'Dead' (from PI channel) labels.\n",
    "    \n",
    "    Returns a list of problematic track IDs and a summary count.\n",
    "    \"\"\"\n",
    "    # Find the first source for each track\n",
    "    first_source_per_track = (\n",
    "        spots_df.sort_values(['TRACK_ID', 'FRAME'])\n",
    "        .groupby('TRACK_ID')\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Identify bad tracks\n",
    "    bad_tracks = first_source_per_track[\n",
    "        (first_source_per_track['SOURCE'] == 'Dead') &\n",
    "        (first_source_per_track['FRAME'] > 0)\n",
    "    ]['TRACK_ID'].tolist()\n",
    "\n",
    "    return bad_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd670e6-f180-4cea-8854-3e424940af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correct path to save spots.csv with re-mapped labels\n",
    "def find_output_folder(path, keyword):\n",
    "    current = path\n",
    "    visited = set()\n",
    "    \n",
    "    while True:\n",
    "        for root, dirs, _ in os.walk(current):\n",
    "            for d in dirs:\n",
    "                if keyword in d and os.path.join(root, d) not in visited:\n",
    "                    return os.path.join(root, d)\n",
    "            visited.update(os.path.join(root, d) for d in dirs)\n",
    "            break  # only recurse 1 level per upward step\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise ValueError(f\"No folder containing keyword '{keyword}' found while walking up from: {path}\")\n",
    "        current = parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b89efc8-3f80-45b9-9c97-79e27b0a5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch concatenate\n",
    "master_folder = '/home/vil945/live_cell_imaging/2025-06-18_live_cell_imaging'\n",
    "\n",
    "# Recursively go through subfolders within master_folder\n",
    "for root, dirs, files in os.walk(master_folder):\n",
    "    # Labeled images to be concatenated should be saved in folders that contain \"filtered_segmentation\" in the name\n",
    "    if \"filtered_segmentation\" in root:\n",
    "        path = Path(root)\n",
    "        tiff_files = list(path.glob(\"*.tif\"))\n",
    "    \n",
    "# Match pairs of iRFP670 and PI segmentations by matching index i within their file names \"xy[i]_iRFP670\", \"xy[i]_PI\"\n",
    "\n",
    "# Dictionaries: key = index i, value = file path\n",
    "iRFP_files = {}\n",
    "PI_files = {}\n",
    "\n",
    "for f in tiff_files:\n",
    "    iRFP_match = re.search(r'xy(\\d+)_iRFP670', f.name)\n",
    "    PI_match = re.search(r'xy(\\d+)_PI', f.name)\n",
    "    if iRFP_match:\n",
    "        idx = iRFP_match.group(1)\n",
    "        iRFP_files[idx] = f\n",
    "    if PI_match:\n",
    "        idx = PI_match.group(1)\n",
    "        PI_files[idx] = f\n",
    "\n",
    "matched_indices = sorted(set(iRFP_files.keys()) & set(PI_files.keys()))\n",
    "\n",
    "for i in matched_indices:\n",
    "    iRFP_file = iRFP_files[i]\n",
    "    PI_file = PI_files[i]\n",
    "    \n",
    "    # Call concatenate label function\n",
    "    combined_lbl, lookup_entries = concat_labels_generate_lookup(iRFP_file, PI_file)\n",
    "    \n",
    "    # Save results\n",
    "    save_dir = path / \"concatenated_segmentation\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    concatenated_name = f\"xy{i}_target_concatenated.tif\"\n",
    "    lookup_name = f\"xy{i}_lookup.csv\"\n",
    "\n",
    "    tifffile.imwrite(save_dir / concatenated_name, combined_lbl.astype(np.uint16))\n",
    "    lookup_df = pd.DataFrame(lookup_entries)\n",
    "    lookup_df.to_csv(save_dir / lookup_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88357ab0-d9b2-4826-8ab6-4cc7e417495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch relabel\n",
    "# Before running this cell, run TrackMate on the concatenated label image, save the spots and tracks csv files in the \"concatenated_segmentation\" folder\n",
    "master_folder = '/home/vil945/live_cell_imaging/2025-06-18_live_cell_imaging'\n",
    "\n",
    "# DO NOT change anything beyond this line\n",
    "# Recursively go through subfolders within master_folder\n",
    "for root, dirs, files in os.walk(master_folder):\n",
    "    if \"concatenated_segmentation\" in root:\n",
    "        path = Path(root)\n",
    "        \n",
    "        # Navigate back up the hierarchy to find the folder to save the outputs\n",
    "        anchor_folder = find_output_folder(path, 'tracking')\n",
    "        \n",
    "        output_folder = os.path.join(anchor_folder, 'combined_spots_relabeled')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        csv_files = list(path.glob(\"*.csv\"))\n",
    "        tiff_files = list(path.glob(\"*.tif\"))\n",
    "    \n",
    "# Match tiplets of (1)spots csv, (2)lookup csv, (3)labels tiff by matching index i within their file names \"xy[i]_combined_spots\", \"xy[i]_lookup\", \"xy[i]_target_concatenated\"\n",
    "\n",
    "# Dictionaries: key = index i, value = file path\n",
    "spots_files = {}\n",
    "lookup_files = {}\n",
    "concat_files = {}\n",
    "\n",
    "for f in csv_files:\n",
    "    spots_match = re.search(r'xy(\\d+)_target_combined_spots', f.name)\n",
    "    lookup_match = re.search(r'xy(\\d+)_lookup', f.name)\n",
    "    if spots_match:\n",
    "        idx = spots_match.group(1)\n",
    "        spots_files[idx] = f\n",
    "    if lookup_match:\n",
    "        idx = lookup_match.group(1)\n",
    "        lookup_files[idx] = f\n",
    "for f in tiff_files:\n",
    "    concat_match = re.search(r'xy(\\d+)_target_concatenated', f.name)\n",
    "    if concat_match:\n",
    "        idx = concat_match.group(1)\n",
    "        concat_files[idx] = f\n",
    "\n",
    "matched_indices = sorted(set(spots_files.keys()) & set(lookup_files.keys()) & set(concat_files.keys()))\n",
    "\n",
    "for i in matched_indices:\n",
    "    spots_file = spots_files[i]\n",
    "    lookup_file = lookup_files[i]\n",
    "    concat_file = concat_files[i]\n",
    "    \n",
    "    # Call remap label function\n",
    "    spots_relabeled = spots_relabel(spots_file, concat_file, lookup_file)\n",
    "\n",
    "    spots_relabeled_name = f\"xy{i}_target_combined_spots_relabeled.csv\"\n",
    "\n",
    "    spots_relabeled.to_csv(os.path.join(output_folder, spots_relabeled_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b8abc14-d35c-4bae-94b2-8dad813e97e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 38.22% (bad transitions) 7.96% (all dead) passed, will exclude errornous tracks and proceed\n",
      "Error rate: 15.38% (bad transitions) 8.33% (all dead) passed, will exclude errornous tracks and proceed\n",
      "Error rate: 19.46% (bad transitions) 11.07% (all dead) passed, will exclude errornous tracks and proceed\n",
      "Error rate: 14.80% (bad transitions) 22.38% (all dead) passed, will exclude errornous tracks and proceed\n"
     ]
    }
   ],
   "source": [
    "# Batch quality check and exlusion of biologically implausible tracks\n",
    "master_folder = '/home/vil945/live_cell_imaging/2025-06-18_live_cell_imaging'\n",
    "\n",
    "# DO NOT change anything beyond this line\n",
    "# Recursively go through subfolders within master_folder\n",
    "for root, dirs, files in os.walk(master_folder):\n",
    "    if \"combined_spots_relabeled\" in root:\n",
    "        path = Path(root)\n",
    "        spots_files = list(path.glob(\"*.csv\"))\n",
    "        \n",
    "        for spots_file in spots_files:\n",
    "            spots_df = pd.read_csv(spots_file)\n",
    "            bad_tracks_transition = tracking_quality_check_transition(spots_df) # Call quality check function to check for implausible Dead â†’ Live transitions\n",
    "            bad_tracks_dead = tracking_quality_check_dead(spots_df) # Call quality check function to check for all dead tracks\n",
    "            error_rate_transition = len(bad_tracks_transition) / spots_df[\"TRACK_ID\"].nunique() * 100\n",
    "            error_rate_dead = len(bad_tracks_dead) / spots_df[\"TRACK_ID\"].nunique() * 100\n",
    "\n",
    "            if error_rate_transition <= 50 and error_rate_dead <= 80:\n",
    "                print(f\"Error rate: {error_rate_transition:.2f}% (bad transitions) {error_rate_dead:.2f}% (all dead) passed, will exclude errornous tracks and proceed\")\n",
    "\n",
    "                clean_spots_df = spots_df[\n",
    "                    ~spots_df[\"TRACK_ID\"].isin(bad_tracks_transition) & \n",
    "                    ~spots_df[\"TRACK_ID\"].isin(bad_tracks_dead)\n",
    "                ].copy()\n",
    "                clean_spots_df.reset_index(drop=True, inplace=True)\n",
    "                clean_spots_df.to_csv(spots_file, index=False) # Overwrites the original spots csv file\n",
    "\n",
    "            else:\n",
    "                print(f\"Error rate: {error_rate_transition:.2f}% (bad transitions) {error_rate_dead:.2f}% (all dead) failed, please check segmentation and/or tracking results and retry\") # Flags high error rate and proceed\n",
    "                continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fd597-de2b-4b58-9252-1a2a92cfc530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
